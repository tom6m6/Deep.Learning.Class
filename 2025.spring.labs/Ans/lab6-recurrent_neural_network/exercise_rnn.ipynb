{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "from rnn_hard_version import Sequence_Modeling\n",
    "from rnn_easy_version import Sequence_Modeling as Sequence_Modeling_pytorch\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 只需要在第一次运行该代码，下载成功之后无需再运行（可能一开始下载不成功或者太慢可多尝试几次）\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(2022)\n",
    "STOPWORDS = stopwords.words(\"english\")\n",
    "PUNCTUATION = '''!\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_jaychou_lyrics():\n",
    "    with open('input/jaychou_train.txt') as f:\n",
    "        train_chars = f.readlines()\n",
    "    train_chars = (''.join(train_chars)).replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "    with open('input/jaychou_test_X.txt') as f:\n",
    "        test_chars = f.readlines()\n",
    "    test_chars = [sent.replace('\\n', '') for sent in test_chars]\n",
    "\n",
    "    return train_chars, test_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def text_processing(train_chars, test_chars, num_steps):\n",
    "    word2idx, idx2word = {'PAD': 0, 'UNK': 1}, {0: 'PAD', 1: 'UNK'}\n",
    "    train_ids, train_labels, test_ids, test_labels = [], [], [], []\n",
    "\n",
    "    for w in train_chars:\n",
    "        if w not in word2idx:\n",
    "            word2idx[w] = len(word2idx)\n",
    "            idx2word[word2idx[w]] = w\n",
    "        train_id = word2idx[w] if w in word2idx else word2idx['UNK']\n",
    "        train_ids += [train_id]\n",
    "\n",
    "    for sent in test_chars:\n",
    "        if len(sent) != num_steps: print(sent); print(len(sent)); exit()\n",
    "        test_id = [word2idx[w] if w in word2idx else word2idx['UNK'] for w in sent]\n",
    "        test_ids += [test_id]\n",
    "\n",
    "    return train_ids, test_ids, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps):\n",
    "    corpus_indices = torch.tensor(corpus_indices, dtype=torch.float32)\n",
    "    data_len = len(corpus_indices)\n",
    "    batch_len = data_len // batch_size\n",
    "    indices = corpus_indices[0: batch_size*batch_len].view(batch_size, batch_len)\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    for i in range(epoch_size):\n",
    "        i = i * num_steps\n",
    "        X = indices[:, i: i + num_steps]\n",
    "        Y = indices[:, i + 1: i + num_steps + 1]\n",
    "        yield torch.tensor(X, dtype=torch.int), torch.tensor(Y, dtype=torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def predict_rnn_pytorch(prefix, num_chars, model, state, idx_to_char, char_to_idx):\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    for t in range(num_chars + len(prefix) - 1):\n",
    "        X = torch.tensor([output[-1]]).view(1, 1)\n",
    "\n",
    "        Y, state = model(X, state)\n",
    "        if t < len(prefix) - 1:\n",
    "            output.append(char_to_idx[prefix[t + 1]])\n",
    "        else:\n",
    "            output.append(int(Y.squeeze(1).argmax(dim=1).item()))\n",
    "    return ''.join([idx_to_char[i] for i in output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def init_rnn_state(batch_size, num_hiddens):\n",
    "    return torch.zeros((1, batch_size, num_hiddens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_with_RNN_hard(train_chars, test_chars):\n",
    "    batch_size, num_hiddens, num_steps = 64, 256, 35\n",
    "    train_ids, test_ids, word2idx, idx2word = text_processing(train_chars, test_chars, num_steps)\n",
    "\n",
    "    model = Sequence_Modeling(len(word2idx), 300, len(word2idx), num_hiddens) #\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    num_epochs = 30\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        state = init_rnn_state(batch_size, num_hiddens)\n",
    "        train_dataloader = data_iter_consecutive(train_ids, batch_size, num_steps)\n",
    "        train_l_sum, train_acc_sum, n = 0., 0., 0\n",
    "        for i, Xy in enumerate(train_dataloader):\n",
    "            state.detach_()\n",
    "            X, y = Xy\n",
    "            # print(X.shape, y.shape, state[0].shape)\n",
    "            y_hat, state = model(X, state)\n",
    "            # print(y_hat.size(), y.size())\n",
    "            y_hat = y_hat.view(y_hat.size(0)*y_hat.size(1), -1)\n",
    "            y = y.view(-1)\n",
    "            loss = loss_func(y_hat, y.long()).sum()\n",
    "\n",
    "            # 梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += loss.item() * y.size(0)\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).float().sum().item()\n",
    "            n += y.size(0)\n",
    "\n",
    "        print('epoch %d, perplexity %.4f, train acc %.3f'\n",
    "              % (epoch, math.exp(train_l_sum / n), train_acc_sum / n))\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            pred_len, prefixes = 50, ['分开', '不分开']\n",
    "            state = init_rnn_state(1, num_hiddens)\n",
    "            for prefix in prefixes:\n",
    "                print(' -', predict_rnn_pytorch(\n",
    "                    prefix, pred_len, model, state, idx2word, word2idx))\n",
    "\n",
    "    # 预测\n",
    "    num_batches = math.ceil(len(test_ids) / batch_size)\n",
    "    pred_txt = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_batches):\n",
    "            batch_data = test_ids[i * batch_size : (i + 1) * batch_size]\n",
    "            X = torch.tensor(batch_data, dtype=torch.int)\n",
    "            state = init_rnn_state(X.size(0), num_hiddens)  # 每个 batch 初始化对应大小的 state\n",
    "            y_hat, _ = model(X, state)\n",
    "            y_hat = y_hat[:, -1, :].argmax(dim=1).tolist()\n",
    "            pred_txt.extend([idx2word[w] for w in y_hat])\n",
    "\n",
    "    with open('output/predict.txt', 'w') as f:\n",
    "        f.write('\\n'.join(pred_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_with_RNN_easy(train_chars, test_chars):\n",
    "    batch_size, num_hiddens, num_steps = 64, 256, 35\n",
    "    train_ids, test_ids, word2idx, idx2word = text_processing(train_chars, test_chars, num_steps)\n",
    "\n",
    "    model = Sequence_Modeling_pytorch(len(word2idx), 300, len(word2idx), num_hiddens) #\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    num_epochs = 60\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        state = init_rnn_state(batch_size, num_hiddens)\n",
    "        train_dataloader = data_iter_consecutive(train_ids, batch_size, num_steps)\n",
    "        train_l_sum, train_acc_sum, n = 0., 0., 0\n",
    "        for i, Xy in enumerate(train_dataloader):\n",
    "            state.detach_()\n",
    "            X, y = Xy\n",
    "            # print(X.shape, y.shape, state[0].shape)\n",
    "            y_hat, state = model(X, state)\n",
    "            # print(y_hat.size(), y.size())\n",
    "            y_hat = y_hat.view(y_hat.size(0)*y_hat.size(1), -1)\n",
    "            y = y.view(-1)\n",
    "            loss = loss_func(y_hat, y.long()).sum()\n",
    "\n",
    "            # 梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += loss.item() * y.size(0)\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).float().sum().item()\n",
    "            n += y.size(0)\n",
    "\n",
    "        print('epoch %d, perplexity %.4f, train acc %.3f'\n",
    "              % (epoch, math.exp(train_l_sum / n), train_acc_sum / n))\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            pred_len, prefixes = 50, ['分开', '不分开']\n",
    "            state = init_rnn_state(1, num_hiddens)\n",
    "            for prefix in prefixes:\n",
    "                print(' -', predict_rnn_pytorch(\n",
    "                    prefix, pred_len, model, state, idx2word, word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def  zip_fun():\n",
    "    path=os.getcwd()\n",
    "    newpath=path+\"/output/\"\n",
    "    os.chdir(newpath)\n",
    "    os.system('zip prediction.zip predict.txt')\n",
    "    os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-14f7b941bf2f>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  yield torch.tensor(X, dtype=torch.int), torch.tensor(Y, dtype=torch.int)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_chars, test_chars = load_jaychou_lyrics()\n",
    "\n",
    "    train_with_RNN_hard(train_chars, test_chars)\n",
    "    \n",
    "    zip_fun()\n",
    "    #train_with_RNN_easy(train_chars, test_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
