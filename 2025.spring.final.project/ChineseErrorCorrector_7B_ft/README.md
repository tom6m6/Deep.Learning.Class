# ä¸­æ–‡æ‹¼å†™å’Œè¯­æ³•çº é”™

[**ğŸ‡¨ğŸ‡³ä¸­æ–‡**](https://github.com/TW-NLP/ChineseErrorCorrector/blob/main/README.md)

<div align="center">
  <a href="https://github.com/TW-NLP/ChineseErrorCorrector">
    <img src="images/image_fx_.jpg" alt="Logo" height="156">
  </a>
</div>



-----------------

## ä»‹ç»

æ”¯æŒä¸­æ–‡æ‹¼å†™å’Œè¯­æ³•é”™è¯¯çº æ­£ï¼Œå¹¶å¼€æºæ‹¼å†™å’Œè¯­æ³•é”™è¯¯çš„å¢å¼ºå·¥å…·ã€å¤§æ¨¡å‹è®­ç»ƒä»£ç ã€‚è£è·2024CCL å† å†›
ğŸ†ï¼Œ[æŸ¥çœ‹è®ºæ–‡](https://aclanthology.org/2024.ccl-3.31/) ï¼Œ[2023 NLPCC-NaCGECçº é”™å† å†›ğŸ†](https://github.com/TW-NLP/ChineseErrorCorrector?tab=readme-ov-file#nacgec-%E6%95%B0%E6%8D%AE%E9%9B%86)ï¼Œ [2022 FCGEC çº é”™å† å†›ğŸ†](https://github.com/TW-NLP/ChineseErrorCorrector?tab=readme-ov-file#fcgec-%E6%95%B0%E6%8D%AE%E9%9B%86)
ï¼Œå¦‚æœ‰å¸®åŠ©ï¼Œæ„Ÿè°¢starâœ¨ã€‚

## ğŸ”¥ğŸ”¥ğŸ”¥ æ–°é—»

[2025/04/28] æ ¹æ®[å»ºè®®](https://github.com/TW-NLP/ChineseErrorCorrector/issues/17)
ï¼Œæˆ‘ä»¬é‡æ–°è®­ç»ƒçº é”™æ¨¡å‹ï¼Œå¹¶å®Œå…¨å¼€æºè®­ç»ƒæ­¥éª¤ï¼Œæ”¯æŒç»“æœå¤ç°ï¼Œ[å¤ç°æ•™ç¨‹](https://github.com/TW-NLP/ChineseErrorCorrector/tree/main?tab=readme-ov-file#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E5%A4%8D%E7%8E%B0)

[2025/03/17]
æ›´æ–°æ‰¹é‡é”™è¯¯æ–‡æœ¬çš„è§£æï¼Œ[transformersæ‰¹é‡è§£æ](https://github.com/TW-NLP/ChineseErrorCorrector?tab=readme-ov-file#transformers-%E6%89%B9%E9%87%8F%E6%8E%A8%E7%90%86) ;[VLLMæ‰¹é‡è§£æ](https://github.com/TW-NLP/ChineseErrorCorrector?tab=readme-ov-file#vllm-%E5%BC%82%E6%AD%A5%E6%89%B9%E9%87%8F%E6%8E%A8%E7%90%86)

[2025/03/10] æ¨¡å‹æ”¯æŒå¤šç§æ¨ç†æ–¹å¼ï¼ŒåŒ…æ‹¬ transformersã€VLLMã€modelscopeã€‚

[2025/02/25]
ğŸ‰ğŸ‰ğŸ‰ä½¿ç”¨200ä¸‡çº é”™æ•°æ®è¿›è¡Œå¤šè½®è¿­ä»£è®­ç»ƒï¼Œå‘å¸ƒäº†[twnlp/ChineseErrorCorrector2-7B](https://huggingface.co/twnlp/ChineseErrorCorrector2-7B)
ï¼Œåœ¨ [NaCGEC-2023NLPCCå®˜æ–¹è¯„æµ‹æ•°æ®é›†](https://github.com/masr2000/NaCGEC)
ä¸Šï¼Œè¶…è¶Šç¬¬ä¸€ååä¸º10ä¸ªç‚¹ï¼Œé¥é¥é¢†å…ˆï¼Œæ¨èä½¿ç”¨âœ¨âœ¨ï¼Œ [æŠ€æœ¯è¯¦æƒ…](https://blog.csdn.net/qq_43765734/article/details/145858955)

[2025/02]
ä¸ºæ–¹ä¾¿éƒ¨ç½²ï¼Œä½¿ç”¨38ä¸‡å¼€æºæ‹¼å†™æ•°æ®ï¼Œå‘å¸ƒäº†[twnlp/ChineseErrorCorrector-1.5B](https://huggingface.co/twnlp/ChineseErrorCorrector-1.5B)

[2025/01]
ä½¿ç”¨38ä¸‡å¼€æºæ‹¼å†™æ•°æ®ï¼ŒåŸºäºQwen2.5è®­ç»ƒä¸­æ–‡æ‹¼å†™çº é”™æ¨¡å‹ï¼Œæ”¯æŒè¯­ä¼¼ã€å½¢ä¼¼ç­‰é”™è¯¯çº æ­£ï¼Œå‘å¸ƒäº†[twnlp/ChineseErrorCorrector-7B](https://huggingface.co/twnlp/ChineseErrorCorrector-7B)ï¼Œ[twnlp/ChineseErrorCorrector-32B-LORA](https://huggingface.co/twnlp/ChineseErrorCorrector-32B-LORA/tree/main)

[2024/06]
v0.1.0ç‰ˆæœ¬ï¼šğŸ‰ğŸ‰ğŸ‰å¼€æºä¸€é”®è¯­æ³•é”™è¯¯å¢å¼ºå·¥å…·ï¼Œè¯¥å·¥å…·å¯ä»¥è¿›è¡Œ14ç§è¯­æ³•é”™è¯¯çš„å¢å¼ºï¼Œä¸åŒè¡Œä¸šå¯ä»¥æ ¹æ®è‡ªå·±çš„æ•°æ®è¿›è¡Œé”™è¯¯æ›¿æ¢ï¼Œæ¥è®­ç»ƒè‡ªå·±çš„è¯­æ³•å’Œæ‹¼å†™æ¨¡å‹ã€‚è¯¦è§[Tag-v0.1.0](https://github.com/TW-NLP/ChineseErrorCorrector/tree/0.1.0)

## æ¨¡å‹åˆ—è¡¨

| æ¨¡å‹åç§°                                                                                        | çº é”™ç±»å‹  | æè¿°                                        |
|:--------------------------------------------------------------------------------------------|:------|:------------------------------------------|
| [twnlp/ChineseErrorCorrector2-7B](https://huggingface.co/twnlp/ChineseErrorCorrector2-7B)   | è¯­æ³•+æ‹¼å†™ | ä½¿ç”¨200ä¸‡çº é”™æ•°æ®è¿›è¡Œå¤šè½®è¿­ä»£è®­ç»ƒï¼Œé€‚ç”¨äºè¯­æ³•çº é”™å’Œæ‹¼å†™çº é”™ï¼Œæ•ˆæœå¥½ï¼Œæ¨èä½¿ç”¨ã€‚ |
| [twnlp/ChineseErrorCorrector-7B](https://huggingface.co/twnlp/ChineseErrorCorrector-7B)     | æ‹¼å†™    | ä½¿ç”¨38ä¸‡å¼€æºæ‹¼å†™æ•°æ®ï¼Œæ”¯æŒè¯­ä¼¼ã€å½¢ä¼¼ç­‰æ‹¼å†™é”™è¯¯çº æ­£ï¼Œæ‹¼å†™çº é”™æ•ˆæœå¥½ã€‚       |
| [twnlp/ChineseErrorCorrector-1.5B](https://huggingface.co/twnlp/ChineseErrorCorrector-1.5B) | æ‹¼å†™    | ä½¿ç”¨38ä¸‡å¼€æºæ‹¼å†™æ•°æ®ï¼Œæ”¯æŒè¯­ä¼¼ã€å½¢ä¼¼ç­‰æ‹¼å†™é”™è¯¯çº æ­£ï¼Œæ‹¼å†™çº é”™æ•ˆæœä¸€èˆ¬ã€‚      |

## æ•°æ®é›†

| æ•°æ®é›†åç§°                        | æ•°æ®é“¾æ¥                                                                                             | æ•°æ®é‡å’Œç±»åˆ«è¯´æ˜                                                                 | æè¿°                              |
|:-----------------------------|:-------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------|:--------------------------------|
| ChinseseErrorCorrectData     | [twnlp/ChinseseErrorCorrectData](https://huggingface.co/datasets/twnlp/ChinseseErrorCorrectData) | 200ä¸‡                                                                     | ChineseErrorCorrector2-7B è®­ç»ƒæ•°æ®é›† |
| CSCï¼ˆæ‹¼å†™çº é”™æ•°æ®é›†ï¼‰                 | [twnlp/csc_data](https://huggingface.co/datasets/twnlp/csc_data)                                 | W271K(279,816) Medical(39,303) Lemon(22,259) ECSpell(6,688) CSCD(35,001) | ä¸­æ–‡æ‹¼å†™çº é”™çš„æ•°æ®é›†                      |
| CGCï¼ˆè¯­æ³•çº é”™æ•°æ®é›†ï¼‰                 | [twnlp/cgc_data](https://huggingface.co/datasets/twnlp/cgc_data)                                 | CGED(20,449) FCGEC(37,354) MuCGEC(2,467) NaSGEC(7,568)                   | ä¸­æ–‡è¯­æ³•çº é”™çš„æ•°æ®é›†                      |
| Lang8+HSKï¼ˆç™¾ä¸‡è¯­æ–™-æ‹¼å†™å’Œè¯­æ³•é”™è¯¯æ··åˆæ•°æ®é›†ï¼‰ | [twnlp/lang8_hsk](https://huggingface.co/datasets/twnlp/lang8_hsk)                               | 1,568,885                                                                | ä¸­æ–‡æ‹¼å†™å’Œè¯­æ³•æ•°æ®é›†                      |

## æ‹¼å†™çº é”™è¯„æµ‹

- è¯„ä¼°æŒ‡æ ‡ï¼šF1

| Model Name                           | Model Link                                                                           | Base Model                 | Avg   | SIGHAN-2015(é€šç”¨) | EC-LAW(æ³•å¾‹) | EC-MED(åŒ»ç–—) | EC-ODW(å…¬æ–‡) |
|:-------------------------------------|:-------------------------------------------------------------------------------------|:---------------------------|:------|:----------------|:-----------|:-----------|:-----------|
| twnlp/ChineseErrorCorrector-1.5B     | [huggingface](https://huggingface.co/twnlp/ChineseErrorCorrector-1.5B/tree/main)     | Qwen/Qwen2.5-1.5B-Instruct | 0.459 | 0.346           | 0.517      | 0.433      | 0.540      |
| twnlp/ChineseErrorCorrector-7B       | [huggingface](https://huggingface.co/twnlp/ChineseErrorCorrector-7B/tree/main)       | Qwen/Qwen2.5-7B-Instruct   | 0.712 | 0.592           | 0.787      | 0.677      | 0.793      |
| twnlp/ChineseErrorCorrector-32B-LORA | [huggingface](https://huggingface.co/twnlp/ChineseErrorCorrector-32B-LORA/tree/main) | Qwen/Qwen2.5-32B-Instruct  | 0.757 | 0.594           | 0.776      | 0.794      | 0.864      |

## æ–‡æœ¬çº é”™è¯„æµ‹(åŒå† å†› ğŸ†)

### NaCGEC æ•°æ®é›†

- è¯„ä¼°å·¥å…·ï¼šChERRANT  [è¯„æµ‹å·¥å…·](https://github.com/HillZhang1999/MuCGEC)
- è¯„ä¼°æ•°æ®ï¼š[NaCGEC](https://github.com/masr2000/NaCGEC)
- è¯„ä¼°æŒ‡æ ‡ï¼šF1-0.5

ğŸ†
| Model Name | Model Link | Prec | Rec | F0.5 |
|:-----------------|:---------------------------------------------------------------|:-----------|:------------|:-------|
| twnlp/ChineseErrorCorrector2-7B | [huggingface](https://huggingface.co/twnlp/ChineseErrorCorrector2-7B) ï¼› [modelspose(å›½å†…ä¸‹è½½)](https://www.modelscope.cn/models/tiannlp/ChineseErrorCorrector2-7B) | 0.5686 | 0.57 | 0.5689 |
| HW_TSC_nlpcc2023_cgec(åä¸º) | æœªå¼€æº | 0.5095 | 0.3129 | 0.4526 |
| é±¼é¥¼å•¾å•¾Plus(åŒ—äº¬å¤§å­¦) | æœªå¼€æº | 0.5708 | 0.1294 | 0.3394 |
| CUHK_SU(é¦™æ¸¯ä¸­æ–‡å¤§å­¦) | æœªå¼€æº | 0.3882 | 0.1558 | 0.2990 |

### FCGEC æ•°æ®é›†

- è¯„ä¼°æŒ‡æ ‡ï¼šbinary_f1

[è¯„æµ‹ğŸ†](https://codalab.lisn.upsaclay.fr/competitions/8020#results)

## ä½¿ç”¨

### ğŸ¤— transformers

```shell
pip install transformers
```

```shell
from transformers import AutoModelForCausalLM, AutoTokenizer,set_seed
set_seed(42)

model_name = "twnlp/ChineseErrorCorrector2-7B"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')

prompt = "ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬çº é”™ä¸“å®¶ï¼Œçº æ­£è¾“å…¥å¥å­ä¸­çš„è¯­æ³•é”™è¯¯ï¼Œå¹¶è¾“å‡ºæ­£ç¡®çš„å¥å­ï¼Œè¾“å…¥å¥å­ä¸ºï¼š"
text_input = "å¯¹å¾…æ¯ä¸€é¡¹å·¥ä½œéƒ½è¦ä¸€ä¸ä¸å¤Ÿã€‚"
messages = [
    {"role": "user", "content": prompt + text_input}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)

```

### VLLM

```shell
pip install transformers
pip install vllm==0.3.3
```

```shell
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

# Initialize the tokenizer
tokenizer = AutoTokenizer.from_pretrained("twnlp/ChineseErrorCorrector2-7B")

# Pass the default decoding hyperparameters of twnlp/ChineseErrorCorrector2-7B
# max_tokens is for the maximum length for generation.
sampling_params = SamplingParams(seed=42,max_tokens=512)

# Input the model name or path. Can be GPTQ or AWQ models.
llm = LLM(model="twnlp/ChineseErrorCorrector2-7B")

# Prepare your prompts
text_input = "å¯¹å¾…æ¯ä¸€é¡¹å·¥ä½œéƒ½è¦ä¸€ä¸ä¸å¤Ÿã€‚"
messages = [
    {"role": "user", "content": "ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬çº é”™ä¸“å®¶ï¼Œçº æ­£è¾“å…¥å¥å­ä¸­çš„è¯­æ³•é”™è¯¯ï¼Œå¹¶è¾“å‡ºæ­£ç¡®çš„å¥å­ï¼Œè¾“å…¥å¥å­ä¸ºï¼š"+text_input}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# generate outputs
outputs = llm.generate([text], sampling_params)

# Print the outputs.
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}") 
```

### VLLM å¼‚æ­¥æ‰¹é‡æ¨ç†

- Clone the repo

``` sh
git clone https://github.com/TW-NLP/ChineseErrorCorrector
cd ChineseErrorCorrector
```

- Install Conda: please see https://docs.conda.io/en/latest/miniconda.html
- Create Conda env:

``` sh
conda create -n zh_correct -y python=3.10
conda activate zh_correct
pip install -r requirements.txt
# If you are in mainland China, you can set the mirror as follows:
pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com
```

```sh
# ä¿®æ”¹config.py
#ï¼ˆ1ï¼‰æ ¹æ®ä¸åŒçš„æ¨¡å‹ï¼Œä¿®æ”¹çš„DEFAULT_CKPT_PATHï¼Œé»˜è®¤ä¸ºChineseErrorCorrector2-7B(å°†æ¨¡å‹ä¸‹è½½ï¼Œæ”¾åœ¨ChineseErrorCorrector/pre_model/ChineseErrorCorrector2-7B)
#ï¼ˆ2ï¼‰å°†Qwen2TextCorConfigçš„USE_VLLM = True

#æ‰¹é‡é¢„æµ‹
python main.py
```

### Transformers æ‰¹é‡æ¨ç†

- Clone the repo

``` sh
git clone https://github.com/TW-NLP/ChineseErrorCorrector
cd ChineseErrorCorrector
```

- Install Conda: please see https://docs.conda.io/en/latest/miniconda.html
- Create Conda env:

``` sh
conda create -n zh_correct -y python=3.10
conda activate zh_correct
pip install -r requirements.txt
# If you are in mainland China, you can set the mirror as follows:
pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com
```

``` sh
# ä¿®æ”¹config.py
#ï¼ˆ1ï¼‰æ ¹æ®ä¸åŒçš„æ¨¡å‹ï¼Œä¿®æ”¹çš„DEFAULT_CKPT_PATHï¼Œé»˜è®¤ä¸ºChineseErrorCorrector2-7B
#ï¼ˆ2ï¼‰å°†Qwen2TextCorConfigçš„USE_VLLM = False

#æ‰¹é‡é¢„æµ‹
python main.py

#è¾“å‡ºï¼š
'''
[{'source': 'å¯¹å¾…æ¯ä¸€é¡¹å·¥ä½œéƒ½è¦ä¸€ä¸ä¸å¤Ÿã€‚', 'target': 'å¯¹å¾…æ¯ä¸€é¡¹å·¥ä½œéƒ½è¦ä¸€ä¸ä¸è‹Ÿã€‚', 'errors': [('å¤Ÿ', 'è‹Ÿ', 12)]}, {'source': 'å¤§çº¦åŠä¸ªå°æ—¶å·¦å³', 'target': 'å¤§çº¦åŠä¸ªå°æ—¶', 'errors': [('å·¦å³', '', 6)]}]
'''

```

### ğŸ¤– modelscope

```shell
pip install modelscope
```

```shell
from modelscope import AutoModelForCausalLM, AutoTokenizer

model_name = "tiannlp/ChineseErrorCorrector2-7B"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = "ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬çº é”™ä¸“å®¶ï¼Œçº æ­£è¾“å…¥å¥å­ä¸­çš„è¯­æ³•é”™è¯¯ï¼Œå¹¶è¾“å‡ºæ­£ç¡®çš„å¥å­ï¼Œè¾“å…¥å¥å­ä¸ºï¼š"
text_input = "å¯¹å¾…æ¯ä¸€é¡¹å·¥ä½œéƒ½è¦ä¸€ä¸ä¸å¤Ÿã€‚"
messages = [
    {"role": "user", "content": prompt + text_input}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)

```

## å®éªŒç»“æœå¤ç°

### ç¯å¢ƒå‡†å¤‡

- Clone the repo

``` sh
git clone https://github.com/TW-NLP/ChineseErrorCorrector
cd ChineseErrorCorrector
```

- Install Conda: please see https://docs.conda.io/en/latest/miniconda.html
- Create Conda env:

``` sh
conda create -n zh_correct -y python=3.10
conda activate zh_correct
pip install -r requirements.txt
# If you are in mainland China, you can set the mirror as follows:
pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com
```

### æ•°æ®å’Œæ¨¡å‹çš„å‡†å¤‡

1ã€ä¸‹è½½è®­ç»ƒæ•°æ®é›†ï¼š[twnlp/ChinseseErrorCorrectData](https://huggingface.co/datasets/twnlp/ChinseseErrorCorrectData) ,æ”¾åœ¨
`/data/paper_data` ä¸­ã€‚

2ã€ä¸‹è½½Qwen2.5-7B-Instructï¼š[Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct) ,æ”¾åœ¨`/pre_model`ä¸­

### æ¨¡å‹è®­ç»ƒä¸åˆå¹¶

``` sh

# Lang8+HSK è®­ç»ƒ
bash ./llm/train/run1.sh
bash ./llm/train/merge1.sh

# CGC+CSC æ•°æ®é›†è®­ç»ƒ
bash ./llm/train/run2.sh
bash ./llm/train/merge2.sh

# Nacgec æ•°æ®é›†è®­ç»ƒ
bash ./llm/train/run3.sh
bash ./llm/train/merge3.sh
``` 

## Citation

If this work is helpful, please kindly cite as:

```bibtex

@inproceedings{wei2024ä¸­å°å­¦ä½œæ–‡è¯­æ³•é”™è¯¯æ£€æµ‹,
  title={ä¸­å°å­¦ä½œæ–‡è¯­æ³•é”™è¯¯æ£€æµ‹, ç—…å¥æ”¹å†™ä¸æµç•…æ€§è¯„çº§çš„è‡ªåŠ¨åŒ–æ–¹æ³•ç ”ç©¶},
  author={Wei, Tian},
  booktitle={Proceedings of the 23rd Chinese National Conference on Computational Linguistics (Volume 3: Evaluations)},
  pages={278--284},
  year={2024}
}
```

## Star History

![Star History Chart](https://api.star-history.com/svg?repos=TW-NLP/ChineseErrorCorrector&type=Date)
